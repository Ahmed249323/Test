{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242ea7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV ,StratifiedKFold\n",
    "from sklearn.preprocessing import  StandardScaler , MinMaxScaler ,RobustScaler, FunctionTransformer ,StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    f1_score, balanced_accuracy_score, precision_recall_fscore_support, \n",
    "    roc_auc_score, confusion_matrix, precision_score, recall_score, \n",
    "    accuracy_score, average_precision_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier ,IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d04219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data\\split\\trainval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e19fdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70320.0</td>\n",
       "      <td>-1.440536</td>\n",
       "      <td>-0.157661</td>\n",
       "      <td>1.951210</td>\n",
       "      <td>3.375675</td>\n",
       "      <td>1.076378</td>\n",
       "      <td>-1.049474</td>\n",
       "      <td>0.043954</td>\n",
       "      <td>0.063408</td>\n",
       "      <td>-1.379699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163049</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>0.071452</td>\n",
       "      <td>0.710874</td>\n",
       "      <td>0.425002</td>\n",
       "      <td>0.353786</td>\n",
       "      <td>-0.007267</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>62.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36855.0</td>\n",
       "      <td>-0.268054</td>\n",
       "      <td>-0.026194</td>\n",
       "      <td>1.862894</td>\n",
       "      <td>-1.989876</td>\n",
       "      <td>-0.551961</td>\n",
       "      <td>-0.310759</td>\n",
       "      <td>0.277641</td>\n",
       "      <td>-0.160598</td>\n",
       "      <td>1.790869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201588</td>\n",
       "      <td>1.162627</td>\n",
       "      <td>-0.333368</td>\n",
       "      <td>0.171281</td>\n",
       "      <td>0.136557</td>\n",
       "      <td>-0.488836</td>\n",
       "      <td>0.063452</td>\n",
       "      <td>-0.095439</td>\n",
       "      <td>25.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42436.0</td>\n",
       "      <td>-3.873686</td>\n",
       "      <td>2.279104</td>\n",
       "      <td>0.434901</td>\n",
       "      <td>-3.173945</td>\n",
       "      <td>-0.504830</td>\n",
       "      <td>0.029066</td>\n",
       "      <td>0.757593</td>\n",
       "      <td>-1.322265</td>\n",
       "      <td>2.122661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.771537</td>\n",
       "      <td>-1.354687</td>\n",
       "      <td>0.149311</td>\n",
       "      <td>-0.385015</td>\n",
       "      <td>-0.752701</td>\n",
       "      <td>0.449522</td>\n",
       "      <td>-3.542555</td>\n",
       "      <td>-1.439570</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163977.0</td>\n",
       "      <td>-0.368752</td>\n",
       "      <td>0.922792</td>\n",
       "      <td>1.353228</td>\n",
       "      <td>2.461567</td>\n",
       "      <td>1.451856</td>\n",
       "      <td>2.942073</td>\n",
       "      <td>-0.074470</td>\n",
       "      <td>0.868245</td>\n",
       "      <td>-1.287337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445099</td>\n",
       "      <td>1.458492</td>\n",
       "      <td>-0.033405</td>\n",
       "      <td>-1.841053</td>\n",
       "      <td>-0.690577</td>\n",
       "      <td>0.123031</td>\n",
       "      <td>0.028285</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130811.0</td>\n",
       "      <td>2.258621</td>\n",
       "      <td>-1.431942</td>\n",
       "      <td>-0.567731</td>\n",
       "      <td>-1.431582</td>\n",
       "      <td>-1.627116</td>\n",
       "      <td>-0.924648</td>\n",
       "      <td>-1.228725</td>\n",
       "      <td>-0.224030</td>\n",
       "      <td>-1.030023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137722</td>\n",
       "      <td>0.117330</td>\n",
       "      <td>0.253277</td>\n",
       "      <td>0.026271</td>\n",
       "      <td>-0.289235</td>\n",
       "      <td>-0.179701</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>-0.042692</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   70320.0 -1.440536 -0.157661  1.951210  3.375675  1.076378 -1.049474   \n",
       "1   36855.0 -0.268054 -0.026194  1.862894 -1.989876 -0.551961 -0.310759   \n",
       "2   42436.0 -3.873686  2.279104  0.434901 -3.173945 -0.504830  0.029066   \n",
       "3  163977.0 -0.368752  0.922792  1.353228  2.461567  1.451856  2.942073   \n",
       "4  130811.0  2.258621 -1.431942 -0.567731 -1.431582 -1.627116 -0.924648   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.043954  0.063408 -1.379699  ...  0.163049  0.035337  0.071452  0.710874   \n",
       "1  0.277641 -0.160598  1.790869  ...  0.201588  1.162627 -0.333368  0.171281   \n",
       "2  0.757593 -1.322265  2.122661  ... -0.771537 -1.354687  0.149311 -0.385015   \n",
       "3 -0.074470  0.868245 -1.287337  ...  0.445099  1.458492 -0.033405 -1.841053   \n",
       "4 -1.228725 -0.224030 -1.030023  ... -0.137722  0.117330  0.253277  0.026271   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.425002  0.353786 -0.007267  0.108108   62.38      0  \n",
       "1  0.136557 -0.488836  0.063452 -0.095439   25.95      0  \n",
       "2 -0.752701  0.449522 -3.542555 -1.439570   17.00      0  \n",
       "3 -0.690577  0.123031  0.028285  0.027508    0.00      0  \n",
       "4 -0.289235 -0.179701  0.024791 -0.042692   25.00      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa84ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56960 entries, 0 to 56959\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Time    56960 non-null  float64\n",
      " 1   V1      56960 non-null  float64\n",
      " 2   V2      56960 non-null  float64\n",
      " 3   V3      56960 non-null  float64\n",
      " 4   V4      56960 non-null  float64\n",
      " 5   V5      56960 non-null  float64\n",
      " 6   V6      56960 non-null  float64\n",
      " 7   V7      56960 non-null  float64\n",
      " 8   V8      56960 non-null  float64\n",
      " 9   V9      56960 non-null  float64\n",
      " 10  V10     56960 non-null  float64\n",
      " 11  V11     56960 non-null  float64\n",
      " 12  V12     56960 non-null  float64\n",
      " 13  V13     56960 non-null  float64\n",
      " 14  V14     56960 non-null  float64\n",
      " 15  V15     56960 non-null  float64\n",
      " 16  V16     56960 non-null  float64\n",
      " 17  V17     56960 non-null  float64\n",
      " 18  V18     56960 non-null  float64\n",
      " 19  V19     56960 non-null  float64\n",
      " 20  V20     56960 non-null  float64\n",
      " 21  V21     56960 non-null  float64\n",
      " 22  V22     56960 non-null  float64\n",
      " 23  V23     56960 non-null  float64\n",
      " 24  V24     56960 non-null  float64\n",
      " 25  V25     56960 non-null  float64\n",
      " 26  V26     56960 non-null  float64\n",
      " 27  V27     56960 non-null  float64\n",
      " 28  V28     56960 non-null  float64\n",
      " 29  Amount  56960 non-null  float64\n",
      " 30  Class   56960 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 13.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87f82266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c791e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b21c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(62)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b993840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54e63f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    56809\n",
       "1       89\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67996dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x= 'Class' ,data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31deb128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating High-Impact Features ===\n",
      "Starting advanced feature extraction...\n",
      "Original dataset shape: (56898, 31)\n",
      "Creating anomaly detection features...\n",
      "After anomaly features: (56898, 77)\n",
      "Creating clustering features...\n",
      "After clustering features: (56898, 88)\n",
      "Creating nearest neighbor features...\n",
      "After nearest_neighbors features: (56898, 93)\n",
      "Creating target encoding features...\n",
      "After target_encoding features: (56898, 138)\n",
      "Creating advanced statistical features...\n",
      "After advanced_stats features: (56898, 145)\n",
      "Creating time-based features...\n",
      "After time_based features: (56898, 153)\n",
      "Creating amount-based features...\n",
      "After amount_based features: (56898, 169)\n",
      "Creating network interaction features...\n",
      "After network features: (56898, 199)\n",
      "Cleaning advanced features...\n",
      "Final advanced feature set shape: (56898, 199)\n",
      "Selecting top 100 high-impact features using random_forest...\n",
      "\n",
      "Top 15 most impactful features:\n",
      "173. V17_V14_V10_sum                (importance: 0.064216)\n",
      "183. V17_V12_V10_sum                (importance: 0.061254)\n",
      "175. V17_V14_V16_sum                (importance: 0.060021)\n",
      "86. dist_to_cluster_7              (importance: 0.052520)\n",
      "185. V17_V12_V16_sum                (importance: 0.050625)\n",
      "179. V17_V14_V7_sum                 (importance: 0.045818)\n",
      "177. V17_V14_V3_sum                 (importance: 0.036805)\n",
      "181. V17_V14_V11_sum                (importance: 0.034736)\n",
      "171. V17_V14_V12_sum                (importance: 0.033685)\n",
      "170. V17_V14_V12_product            (importance: 0.031346)\n",
      "70. V12_zscore_strength            (importance: 0.025334)\n",
      "187. V17_V12_V3_sum                 (importance: 0.025297)\n",
      "184. V17_V12_V16_product            (importance: 0.022191)\n",
      "18. V17                            (importance: 0.020781)\n",
      "180. V17_V14_V11_product            (importance: 0.018890)\n",
      "\n",
      "✅ Successfully created 199 total features\n",
      "✅ Selected 100 high-impact features\n",
      "\n",
      "Feature type distribution in top 100 features:\n",
      "  V17: 22 features\n",
      "  amount: 10 features\n",
      "  dist: 7 features\n",
      "  row: 5 features\n",
      "  time: 4 features\n",
      "  V12: 3 features\n",
      "  V10: 3 features\n",
      "  V11: 3 features\n",
      "  rolling: 2 features\n",
      "  V16: 2 features\n",
      "  V14: 2 features\n",
      "  min: 2 features\n",
      "  V13: 2 features\n",
      "  V4: 2 features\n",
      "  Time: 2 features\n",
      "  V8: 2 features\n",
      "  V9: 2 features\n",
      "  V3: 2 features\n",
      "  V2: 2 features\n",
      "  V6: 2 features\n",
      "  cluster: 1 features\n",
      "  avg: 1 features\n",
      "  local: 1 features\n",
      "  V26: 1 features\n",
      "  V27: 1 features\n",
      "  V20: 1 features\n",
      "  V28: 1 features\n",
      "  std: 1 features\n",
      "  isolation: 1 features\n",
      "  V19: 1 features\n",
      "  max: 1 features\n",
      "  V22: 1 features\n",
      "  V5: 1 features\n",
      "  Amount: 1 features\n",
      "  V25: 1 features\n",
      "  V1: 1 features\n",
      "  V21: 1 features\n",
      "  V7: 1 features\n",
      "  V24: 1 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AdvancedFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Advanced feature extraction with focus on target impact\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_column='Class'):\n",
    "        self.target_column = target_column\n",
    "        self.scalers = {}\n",
    "        self.models = {}\n",
    "        \n",
    "    def anomaly_detection_features(self, df):\n",
    "        \"\"\"Create anomaly detection features - highly predictive for fraud detection\"\"\"\n",
    "        print(\"Creating anomaly detection features...\")\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        anomaly_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        try:\n",
    "            # Isolation Forest anomaly scores\n",
    "            iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "            anomaly_scores = iso_forest.fit_predict(df[numerical_cols])\n",
    "            anomaly_features['isolation_forest_score'] = iso_forest.decision_function(df[numerical_cols])\n",
    "            anomaly_features['is_anomaly'] = (anomaly_scores == -1).astype(int)\n",
    "            \n",
    "            # Statistical outlier detection\n",
    "            for col in numerical_cols[:15]:  # Top 15 features\n",
    "                # Z-score based outliers\n",
    "                z_scores = np.abs(stats.zscore(df[col].fillna(df[col].mean())))\n",
    "                anomaly_features[f'{col}_zscore_outlier'] = (z_scores > 3).astype(int)\n",
    "                anomaly_features[f'{col}_zscore_strength'] = z_scores\n",
    "                \n",
    "                # IQR based outliers\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                anomaly_features[f'{col}_iqr_outlier'] = ((df[col] < lower_bound) | (df[col] > upper_bound)).astype(int)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in anomaly detection: {e}\")\n",
    "        \n",
    "        return anomaly_features\n",
    "    \n",
    "    def clustering_features(self, df, n_clusters=8):\n",
    "        \"\"\"Create clustering-based features\"\"\"\n",
    "        print(\"Creating clustering features...\")\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        cluster_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        try:\n",
    "            # Standardize data for clustering\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(df[numerical_cols])\n",
    "            \n",
    "            # K-means clustering\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "            \n",
    "            cluster_features['cluster_id'] = cluster_labels\n",
    "            \n",
    "            # Distance to cluster centers\n",
    "            centers = kmeans.cluster_centers_\n",
    "            distances = cdist(scaled_data, centers)\n",
    "            \n",
    "            for i in range(n_clusters):\n",
    "                cluster_features[f'dist_to_cluster_{i}'] = distances[:, i]\n",
    "            \n",
    "            # Distance to closest cluster center\n",
    "            cluster_features['min_cluster_distance'] = np.min(distances, axis=1)\n",
    "            \n",
    "            # Cluster size (how many points in same cluster)\n",
    "            cluster_sizes = pd.Series(cluster_labels).value_counts().to_dict()\n",
    "            cluster_features['cluster_size'] = pd.Series(cluster_labels).map(cluster_sizes)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in clustering: {e}\")\n",
    "        \n",
    "        return cluster_features\n",
    "    \n",
    "    def nearest_neighbor_features(self, df, n_neighbors=10):\n",
    "        \"\"\"Create nearest neighbor based features\"\"\"\n",
    "        print(\"Creating nearest neighbor features...\")\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        nn_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        try:\n",
    "            # Use most important features for NN\n",
    "            if self.target_column in df.columns:\n",
    "                correlations = df[numerical_cols].corrwith(df[self.target_column]).abs().sort_values(ascending=False)\n",
    "                top_features = correlations.head(10).index.tolist()\n",
    "            else:\n",
    "                top_features = numerical_cols[:10]\n",
    "            \n",
    "            # Standardize data\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = scaler.fit_transform(df[top_features])\n",
    "            \n",
    "            # Nearest neighbors\n",
    "            nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "            nn_model.fit(scaled_data)\n",
    "            \n",
    "            distances, indices = nn_model.kneighbors(scaled_data)\n",
    "            \n",
    "            # Features based on nearest neighbors\n",
    "            nn_features['avg_nn_distance'] = np.mean(distances[:, 1:], axis=1)  # Exclude self\n",
    "            nn_features['min_nn_distance'] = np.min(distances[:, 1:], axis=1)\n",
    "            nn_features['max_nn_distance'] = np.max(distances[:, 1:], axis=1)\n",
    "            nn_features['std_nn_distance'] = np.std(distances[:, 1:], axis=1)\n",
    "            \n",
    "            # Local density estimation\n",
    "            nn_features['local_density'] = 1 / (nn_features['avg_nn_distance'] + 1e-8)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in nearest neighbors: {e}\")\n",
    "        \n",
    "        return nn_features\n",
    "    \n",
    "    def target_encoding_features(self, df):\n",
    "        \"\"\"Create target encoding features (if target is available)\"\"\"\n",
    "        print(\"Creating target encoding features...\")\n",
    "        \n",
    "        if self.target_column not in df.columns:\n",
    "            return pd.DataFrame(index=df.index)\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        target_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        try:\n",
    "            # Binned target encoding\n",
    "            for col in numerical_cols[:15]:\n",
    "                # Create bins for continuous variables\n",
    "                try:\n",
    "                    n_bins = min(10, df[col].nunique())\n",
    "                    if n_bins > 1:\n",
    "                        df[f'{col}_binned'] = pd.qcut(df[col], q=n_bins, duplicates='drop', labels=False)\n",
    "                        \n",
    "                        # Target mean for each bin\n",
    "                        bin_means = df.groupby(f'{col}_binned')[self.target_column].mean()\n",
    "                        target_features[f'{col}_target_mean'] = df[f'{col}_binned'].map(bin_means).fillna(df[self.target_column].mean())\n",
    "                        \n",
    "                        # Target std for each bin\n",
    "                        bin_stds = df.groupby(f'{col}_binned')[self.target_column].std()\n",
    "                        target_features[f'{col}_target_std'] = df[f'{col}_binned'].map(bin_stds).fillna(df[self.target_column].std())\n",
    "                        \n",
    "                        # Count in each bin\n",
    "                        bin_counts = df.groupby(f'{col}_binned').size()\n",
    "                        target_features[f'{col}_bin_count'] = df[f'{col}_binned'].map(bin_counts).fillna(1)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in target encoding: {e}\")\n",
    "        \n",
    "        return target_features\n",
    "    \n",
    "    def advanced_statistical_features(self, df):\n",
    "        \"\"\"Create advanced statistical features\"\"\"\n",
    "        print(\"Creating advanced statistical features...\")\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        advanced_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Row-wise advanced statistics\n",
    "        data_matrix = df[numerical_cols].values\n",
    "        \n",
    "        # Entropy-like measures\n",
    "        advanced_features['row_entropy'] = -np.sum(data_matrix * np.log(np.abs(data_matrix) + 1e-8), axis=1)\n",
    "        \n",
    "        # Coefficient of variation\n",
    "        row_means = np.mean(data_matrix, axis=1)\n",
    "        row_stds = np.std(data_matrix, axis=1)\n",
    "        advanced_features['row_cv'] = row_stds / (row_means + 1e-8)\n",
    "        \n",
    "        # Percentile ranges\n",
    "        advanced_features['row_p90_p10'] = np.percentile(data_matrix, 90, axis=1) - np.percentile(data_matrix, 10, axis=1)\n",
    "        advanced_features['row_p95_p5'] = np.percentile(data_matrix, 95, axis=1) - np.percentile(data_matrix, 5, axis=1)\n",
    "        \n",
    "        # Number of peaks (local maxima)\n",
    "        for i in range(len(df)):\n",
    "            row_data = data_matrix[i, :]\n",
    "            peaks = 0\n",
    "            for j in range(1, len(row_data)-1):\n",
    "                if row_data[j] > row_data[j-1] and row_data[j] > row_data[j+1]:\n",
    "                    peaks += 1\n",
    "            advanced_features.loc[df.index[i], 'row_peaks'] = peaks\n",
    "        \n",
    "        # Monotonicity measures\n",
    "        advanced_features['row_monotonic_increase'] = 0\n",
    "        advanced_features['row_monotonic_decrease'] = 0\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            row_data = data_matrix[i, :]\n",
    "            diffs = np.diff(row_data)\n",
    "            advanced_features.loc[df.index[i], 'row_monotonic_increase'] = (diffs >= 0).sum()\n",
    "            advanced_features.loc[df.index[i], 'row_monotonic_decrease'] = (diffs <= 0).sum()\n",
    "        \n",
    "        return advanced_features\n",
    "    \n",
    "    def time_based_features(self, df):\n",
    "        \"\"\"Create time-based features if Time column exists\"\"\"\n",
    "        print(\"Creating time-based features...\")\n",
    "        \n",
    "        time_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        if 'Time' not in df.columns:\n",
    "            return time_features\n",
    "        \n",
    "        try:\n",
    "            # Time since start\n",
    "            time_features['time_since_start'] = df['Time'] - df['Time'].min()\n",
    "            \n",
    "            # Time until end\n",
    "            time_features['time_until_end'] = df['Time'].max() - df['Time']\n",
    "            \n",
    "            # Time periods (assuming seconds)\n",
    "            time_features['time_hour'] = (df['Time'] / 3600) % 24\n",
    "            time_features['time_day'] = (df['Time'] / (3600 * 24)) % 7\n",
    "            \n",
    "            # Time-based binning\n",
    "            time_bins = pd.qcut(df['Time'], q=10, labels=False, duplicates='drop')\n",
    "            time_features['time_bin'] = time_bins\n",
    "            \n",
    "            # Rolling statistics if we have target\n",
    "            if self.target_column in df.columns:\n",
    "                df_sorted = df.sort_values('Time')\n",
    "                \n",
    "                # Rolling mean of target (fraud rate in time windows)\n",
    "                for window in [100, 500, 1000]:\n",
    "                    rolling_target = df_sorted[self.target_column].rolling(window=window, min_periods=1).mean()\n",
    "                    # Map back to original order\n",
    "                    time_features['rolling_fraud_rate_' + str(window)] = rolling_target.reindex(df.index).fillna(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in time features: {e}\")\n",
    "        \n",
    "        return time_features\n",
    "    \n",
    "    def amount_based_features(self, df):\n",
    "        \"\"\"Create amount-based features if Amount column exists\"\"\"\n",
    "        print(\"Creating amount-based features...\")\n",
    "        \n",
    "        amount_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        if 'Amount' not in df.columns:\n",
    "            return amount_features\n",
    "        \n",
    "        try:\n",
    "            # Amount percentiles\n",
    "            amount_features['amount_percentile'] = df['Amount'].rank(pct=True)\n",
    "            \n",
    "            # Amount z-score\n",
    "            amount_features['amount_zscore'] = (df['Amount'] - df['Amount'].mean()) / (df['Amount'].std() + 1e-8)\n",
    "            \n",
    "            # Amount categories\n",
    "            amount_features['is_zero_amount'] = (df['Amount'] == 0).astype(int)\n",
    "            amount_features['is_small_amount'] = (df['Amount'] < df['Amount'].quantile(0.1)).astype(int)\n",
    "            amount_features['is_large_amount'] = (df['Amount'] > df['Amount'].quantile(0.9)).astype(int)\n",
    "            \n",
    "            # Amount bins\n",
    "            amount_bins = pd.qcut(df['Amount'], q=10, labels=False, duplicates='drop')\n",
    "            amount_features['amount_bin'] = amount_bins\n",
    "            \n",
    "            # Amount vs other features ratios\n",
    "            numerical_cols = [col for col in df.columns if col != self.target_column and col != 'Amount' and df[col].dtype in ['float64', 'int64']]\n",
    "            \n",
    "            if self.target_column in df.columns:\n",
    "                correlations = df[numerical_cols].corrwith(df[self.target_column]).abs().sort_values(ascending=False)\n",
    "                top_features = correlations.head(5).index.tolist()\n",
    "                \n",
    "                for col in top_features:\n",
    "                    amount_features[f'amount_to_{col}_ratio'] = df['Amount'] / (np.abs(df[col]) + 1e-8)\n",
    "                    amount_features[f'amount_minus_{col}'] = df['Amount'] - df[col]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in amount features: {e}\")\n",
    "        \n",
    "        return amount_features\n",
    "    \n",
    "    def interaction_network_features(self, df):\n",
    "        \"\"\"Create complex interaction features\"\"\"\n",
    "        print(\"Creating network interaction features...\")\n",
    "        \n",
    "        numerical_cols = [col for col in df.columns if col != self.target_column and df[col].dtype in ['float64', 'int64']]\n",
    "        network_features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        try:\n",
    "            if self.target_column in df.columns:\n",
    "                # Get top correlated features\n",
    "                correlations = df[numerical_cols].corrwith(df[self.target_column]).abs().sort_values(ascending=False)\n",
    "                top_features = correlations.head(8).index.tolist()\n",
    "                \n",
    "                # Create interaction network\n",
    "                interaction_count = 0\n",
    "                for i, col1 in enumerate(top_features):\n",
    "                    for j, col2 in enumerate(top_features[i+1:], i+1):\n",
    "                        for k, col3 in enumerate(top_features[j+1:], j+1):\n",
    "                            if interaction_count >= 10:  # Limit to avoid too many features\n",
    "                                break\n",
    "                            \n",
    "                            # Three-way interactions\n",
    "                            network_features[f'{col1}_{col2}_{col3}_product'] = df[col1] * df[col2] * df[col3]\n",
    "                            network_features[f'{col1}_{col2}_{col3}_sum'] = df[col1] + df[col2] + df[col3]\n",
    "                            \n",
    "                            interaction_count += 1\n",
    "                            \n",
    "                        if interaction_count >= 10:\n",
    "                            break\n",
    "                    if interaction_count >= 10:\n",
    "                        break\n",
    "                \n",
    "                # Conditional features\n",
    "                for col in top_features[:5]:\n",
    "                    # Features conditional on being above/below median\n",
    "                    median_val = df[col].median()\n",
    "                    above_median = df[col] > median_val\n",
    "                    \n",
    "                    network_features[f'{col}_above_median'] = above_median.astype(int)\n",
    "                    network_features[f'{col}_conditional_mean'] = np.where(above_median, df[col], 0)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning in network features: {e}\")\n",
    "        \n",
    "        return network_features\n",
    "    \n",
    "    def extract_advanced_features(self, df, include_original=True):\n",
    "        \"\"\"Extract all advanced features\"\"\"\n",
    "        print(\"Starting advanced feature extraction...\")\n",
    "        print(f\"Original dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Start with original features\n",
    "        if include_original:\n",
    "            feature_df = df.drop(columns=[self.target_column] if self.target_column in df.columns else []).copy()\n",
    "        else:\n",
    "            feature_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Add advanced features\n",
    "        feature_extractors = [\n",
    "            ('anomaly', self.anomaly_detection_features),\n",
    "            ('clustering', self.clustering_features),\n",
    "            ('nearest_neighbors', self.nearest_neighbor_features),\n",
    "            ('target_encoding', self.target_encoding_features),\n",
    "            ('advanced_stats', self.advanced_statistical_features),\n",
    "            ('time_based', self.time_based_features),\n",
    "            ('amount_based', self.amount_based_features),\n",
    "            ('network', self.interaction_network_features)\n",
    "        ]\n",
    "        \n",
    "        for name, extractor in feature_extractors:\n",
    "            try:\n",
    "                new_features = extractor(df)\n",
    "                if not new_features.empty:\n",
    "                    feature_df = pd.concat([feature_df, new_features], axis=1)\n",
    "                    print(f\"After {name} features: {feature_df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {name} features: {e}\")\n",
    "        \n",
    "        # Clean data\n",
    "        print(\"Cleaning advanced features...\")\n",
    "        feature_df = feature_df.replace([np.inf, -np.inf], np.nan)\n",
    "        feature_df = feature_df.fillna(0)\n",
    "        \n",
    "        # Ensure all numeric\n",
    "        for col in feature_df.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(feature_df[col]):\n",
    "                feature_df[col] = pd.to_numeric(feature_df[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        print(f\"Final advanced feature set shape: {feature_df.shape}\")\n",
    "        return feature_df\n",
    "    \n",
    "    def select_high_impact_features(self, X, y, method='random_forest', k=100):\n",
    "        \"\"\"Select features with highest target impact\"\"\"\n",
    "        print(f\"Selecting top {k} high-impact features using {method}...\")\n",
    "        \n",
    "        if method == 'random_forest':\n",
    "            rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, max_depth=10)\n",
    "            rf.fit(X, y)\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': rf.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 15 most impactful features:\")\n",
    "            for i, row in importance_df.head(15).iterrows():\n",
    "                print(f\"{row.name+1:2d}. {row['feature']:<30} (importance: {row['importance']:.6f})\")\n",
    "            \n",
    "            selected_features = importance_df.head(k)['feature'].tolist()\n",
    "            return X[selected_features], selected_features\n",
    "        \n",
    "        elif method == 'mutual_info':\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            return pd.DataFrame(X_selected, columns=selected_features, index=X.index), selected_features\n",
    "\n",
    "def create_high_impact_features(df, target_column='Class', n_features=100):\n",
    "    \"\"\"\n",
    "    Main function to create high-impact features\n",
    "    \"\"\"\n",
    "    extractor = AdvancedFeatureExtractor(target_column=target_column)\n",
    "    \n",
    "    # Extract advanced features\n",
    "    advanced_features = extractor.extract_advanced_features(df, include_original=True)\n",
    "    \n",
    "    # Select most impactful features if target exists\n",
    "    if target_column in df.columns:\n",
    "        X = advanced_features\n",
    "        y = df[target_column]\n",
    "        \n",
    "        selected_features, feature_names = extractor.select_high_impact_features(\n",
    "            X, y, method='random_forest', k=n_features\n",
    "        )\n",
    "        \n",
    "        return advanced_features, selected_features, feature_names\n",
    "    \n",
    "    return advanced_features, None, advanced_features.columns.tolist()\n",
    "\n",
    "# Usage:\n",
    "print(\"=== Creating High-Impact Features ===\")\n",
    "try:\n",
    "    all_features, selected_features, feature_names = create_high_impact_features(\n",
    "        df, target_column='Class', n_features=100\n",
    "    )\n",
    "    print(f\"\\n✅ Successfully created {all_features.shape[1]} total features\")\n",
    "    print(f\"✅ Selected {selected_features.shape[1] if selected_features is not None else 0} high-impact features\")\n",
    "    \n",
    "    if selected_features is not None:\n",
    "        # Show feature importance distribution\n",
    "        from collections import Counter\n",
    "        feature_types = Counter([name.split('_')[0] for name in feature_names])\n",
    "        print(f\"\\nFeature type distribution in top {len(feature_names)} features:\")\n",
    "        for ftype, count in feature_types.most_common():\n",
    "            print(f\"  {ftype}: {count} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40c3aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=selected_features\n",
    "y= df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "663d74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a287b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outliers(X):\n",
    "    Q1 = np.percentile(X, 25, axis=0)\n",
    "    Q3 = np.percentile(X, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return np.clip(X, lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1cafca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, X_val, y_val, metric='f1'):\n",
    "    \"\"\"Find optimal threshold for binary classification using validation set\"\"\"\n",
    "    if not hasattr(model, \"predict_proba\") or y_val.nunique() != 2:\n",
    "        return 0.5  # Default threshold for non-probabilistic models or multiclass\n",
    "    \n",
    "    probs = model.predict_proba(X_val)[:, 1]\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    best_score = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_val, y_pred, average='macro')\n",
    "        elif metric == 'balanced_accuracy':\n",
    "            score = balanced_accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba27e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def comprehensive_evaluation(model, X, y, threshold=0.5, set_name=\"\", model_name=\"\"):\n",
    "    \"\"\"Comprehensive evaluation for binary imbalanced datasets\"\"\"\n",
    "    \n",
    "    # Get probabilities and predictions\n",
    "    probs = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate all binary classification metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y, y_pred)\n",
    "    \n",
    "    # F1 scores\n",
    "    f1_macro = f1_score(y, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "    f1_binary = f1_score(y, y_pred)  # Binary F1 (focuses on positive class)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision_macro = precision_score(y, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y, y_pred, average='macro')\n",
    "    precision_binary = precision_score(y, y_pred)  # For positive class\n",
    "    recall_binary = recall_score(y, y_pred)  # For positive class (sensitivity)\n",
    "    \n",
    "    # AUC scores\n",
    "    auc_roc = roc_auc_score(y, probs)\n",
    "    auc_pr = average_precision_score(y, probs)  # Area under Precision-Recall curve\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        y, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    # Additional binary metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "    \n",
    "    # Positive and Negative Predictive Values\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Same as precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "    \n",
    "    return {\n",
    "        # Basic metrics\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        \n",
    "        # F1 scores\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_binary': f1_binary,\n",
    "        \n",
    "        # Precision and Recall\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'precision_binary': precision_binary,\n",
    "        'recall_binary': recall_binary,\n",
    "        \n",
    "        # AUC scores\n",
    "        'auc_roc': auc_roc,\n",
    "        'auc_pr': auc_pr,\n",
    "        \n",
    "        # Binary-specific metrics\n",
    "        'sensitivity': sensitivity,  # Same as recall\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,  # Positive Predictive Value (same as precision)\n",
    "        'npv': npv,  # Negative Predictive Value\n",
    "        \n",
    "        # Confusion matrix elements\n",
    "        'true_positives': tp,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        \n",
    "        # Per-class arrays (for compatibility)\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'support': support,\n",
    "        \n",
    "        # Raw outputs\n",
    "        'probabilities': probs,\n",
    "        'predictions': y_pred,\n",
    "        'threshold': threshold\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99376f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Simplified models configuration for imbalanced datasets\n",
    "models = {\n",
    "    'LogisticRegression': (\n",
    "        LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ),\n",
    "        {\n",
    "            'C': [0.1, 1.0, 10.0]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [10, 20]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': (\n",
    "        XGBClassifier(\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            objective='binary:logistic',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [3, 6],\n",
    "            \"learning_rate\": [0.1, 0.2],\n",
    "            \"scale_pos_weight\": [1, 5]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': (\n",
    "        LGBMClassifier(\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            objective='binary',\n",
    "            is_unbalance=True,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.1, 0.2]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    'CatBoost': (\n",
    "        CatBoostClassifier(\n",
    "            random_state=42,\n",
    "            verbose=False,\n",
    "            auto_class_weights='Balanced'\n",
    "        ),\n",
    "        {\n",
    "            \"iterations\": [100, 200],\n",
    "            \"learning_rate\": [0.1, 0.2]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    # Balanced Bagging\n",
    "    'Balanced_Bagging': (\n",
    "        BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "            n_estimators=50,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [50, 100]\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    \n",
    "    # Voting Classifier\n",
    "    'Voting': (\n",
    "        VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42)),\n",
    "                ('xgb', XGBClassifier(n_estimators=50, objective='binary:logistic', random_state=42, eval_metric='logloss'))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {},\n",
    "        False\n",
    "    ),\n",
    "    \n",
    "    # Stacking Classifier\n",
    "    'Stacking': (\n",
    "        StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42)),\n",
    "                ('xgb', XGBClassifier(n_estimators=50, objective='binary:logistic', random_state=42, eval_metric='logloss')),\n",
    "                ('lgbm', LGBMClassifier(n_estimators=50, objective='binary', is_unbalance=True, random_state=42, verbose=-1))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(class_weight='balanced', random_state=42),\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"cv\": [3, 5]\n",
    "        },\n",
    "        True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43a81784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and tuning LogisticRegression...\n",
      "Best params: {'C': 10.0}\n",
      "Best CV score: 0.6567\n",
      "Optimal threshold: 0.8400\n",
      "\n",
      "=== LogisticRegression - Validation Set Results ===\n",
      "🎯 Threshold: 0.8400\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8421\n",
      "   F1-Macro: 0.9209\n",
      "   Balanced Accuracy: 0.9443\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.8000\n",
      "   Recall (Sensitivity): 0.8889\n",
      "   Specificity: 0.9996\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9622\n",
      "   AUC-PR: 0.7885\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,358\n",
      "   False Positives: 4\n",
      "   False Negatives: 2\n",
      "   True Positives:  16\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.90      0.94      0.92     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "Training and tuning Random Forest...\n",
      "Best params: {'max_depth': 10, 'n_estimators': 100}\n",
      "Best CV score: 0.7776\n",
      "Optimal threshold: 0.2300\n",
      "\n",
      "=== Random Forest - Validation Set Results ===\n",
      "🎯 Threshold: 0.2300\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8571\n",
      "   F1-Macro: 0.9285\n",
      "   Balanced Accuracy: 0.9166\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.8824\n",
      "   Recall (Sensitivity): 0.8333\n",
      "   Specificity: 0.9998\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9430\n",
      "   AUC-PR: 0.8527\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,360\n",
      "   False Positives: 2\n",
      "   False Negatives: 3\n",
      "   True Positives:  15\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.94      0.92      0.93     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "Training and tuning XGBoost...\n",
      "Best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 5}\n",
      "Best CV score: 0.8090\n",
      "Optimal threshold: 0.3000\n",
      "\n",
      "=== XGBoost - Validation Set Results ===\n",
      "🎯 Threshold: 0.3000\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8889\n",
      "   F1-Macro: 0.9444\n",
      "   Balanced Accuracy: 0.9444\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.8889\n",
      "   Recall (Sensitivity): 0.8889\n",
      "   Specificity: 0.9998\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9856\n",
      "   AUC-PR: 0.8783\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,360\n",
      "   False Positives: 2\n",
      "   False Negatives: 2\n",
      "   True Positives:  16\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.94      0.94      0.94     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "Training and tuning LightGBM...\n",
      "Best params: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Best CV score: 0.0493\n",
      "Optimal threshold: 0.1000\n",
      "\n",
      "=== LightGBM - Validation Set Results ===\n",
      "🎯 Threshold: 0.1000\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.0069\n",
      "   F1-Macro: 0.3772\n",
      "   Balanced Accuracy: 0.7429\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.0035\n",
      "   Recall (Sensitivity): 0.8889\n",
      "   Specificity: 0.5968\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.7429\n",
      "   AUC-PR: 0.0033\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  6,781\n",
      "   False Positives: 4,581\n",
      "   False Negatives: 2\n",
      "   True Positives:  16\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.60      0.75     11362\n",
      "    Positive       0.00      0.89      0.01        18\n",
      "\n",
      "    accuracy                           0.60     11380\n",
      "   macro avg       0.50      0.74      0.38     11380\n",
      "weighted avg       1.00      0.60      0.75     11380\n",
      "\n",
      "\n",
      "Training and tuning CatBoost...\n",
      "Best params: {'iterations': 100, 'learning_rate': 0.1}\n",
      "Best CV score: 0.8083\n",
      "Optimal threshold: 0.7900\n",
      "\n",
      "=== CatBoost - Validation Set Results ===\n",
      "🎯 Threshold: 0.7900\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8649\n",
      "   F1-Macro: 0.9323\n",
      "   Balanced Accuracy: 0.9443\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.8421\n",
      "   Recall (Sensitivity): 0.8889\n",
      "   Specificity: 0.9997\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9786\n",
      "   AUC-PR: 0.8617\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,359\n",
      "   False Positives: 3\n",
      "   False Negatives: 2\n",
      "   True Positives:  16\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.92      0.94      0.93     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "Training and tuning Balanced_Bagging...\n",
      "Best params: {'n_estimators': 100}\n",
      "Best CV score: 0.7674\n",
      "Optimal threshold: 0.2900\n",
      "\n",
      "=== Balanced_Bagging - Validation Set Results ===\n",
      "🎯 Threshold: 0.2900\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8824\n",
      "   F1-Macro: 0.9411\n",
      "   Balanced Accuracy: 0.9166\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.9375\n",
      "   Recall (Sensitivity): 0.8333\n",
      "   Specificity: 0.9999\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9438\n",
      "   AUC-PR: 0.8280\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,361\n",
      "   False Positives: 1\n",
      "   False Negatives: 3\n",
      "   True Positives:  15\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.97      0.92      0.94     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "Training and tuning Voting...\n",
      "Error training Voting: name 'cross_val_score' is not defined\n",
      "\n",
      "Training and tuning Stacking...\n",
      "Best params: {'cv': 3}\n",
      "Best CV score: 0.7396\n",
      "Optimal threshold: 0.7700\n",
      "\n",
      "=== Stacking - Validation Set Results ===\n",
      "🎯 Threshold: 0.7700\n",
      "\n",
      "📈 Key Metrics:\n",
      "   F1-Binary (Positive Class): 0.8649\n",
      "   F1-Macro: 0.9323\n",
      "   Balanced Accuracy: 0.9443\n",
      "\n",
      "🔍 Positive Class Performance:\n",
      "   Precision: 0.8421\n",
      "   Recall (Sensitivity): 0.8889\n",
      "   Specificity: 0.9997\n",
      "\n",
      "📊 AUC Scores:\n",
      "   AUC-ROC: 0.9433\n",
      "   AUC-PR: 0.8795\n",
      "\n",
      "🔢 Confusion Matrix:\n",
      "   True Negatives:  11,359\n",
      "   False Positives: 3\n",
      "   False Negatives: 2\n",
      "   True Positives:  16\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     11362\n",
      "    Positive       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           1.00     11380\n",
      "   macro avg       0.92      0.94      0.93     11380\n",
      "weighted avg       1.00      1.00      1.00     11380\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF BINARY CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "F1 Scores Summary:\n",
      "                   F1_Binary                    F1_Macro                   \n",
      "Set                     Test   Train Validation     Test   Train Validation\n",
      "Model                                                                      \n",
      "Balanced_Bagging      0.8000  0.9725     0.8824   0.8999  0.9862     0.9411\n",
      "CatBoost              0.8750  0.9464     0.8649   0.9374  0.9732     0.9323\n",
      "LightGBM              0.0077  0.0074     0.0069   0.3766  0.3743     0.3772\n",
      "LogisticRegression    0.7778  0.8689     0.8421   0.8887  0.9343     0.9209\n",
      "Random Forest         0.8750  0.9138     0.8571   0.9374  0.9568     0.9285\n",
      "Stacking              0.8485  0.9550     0.8649   0.9241  0.9774     0.9323\n",
      "XGBoost               0.8750  0.9725     0.8889   0.9374  0.9862     0.9444\n",
      "\n",
      "Key Binary Metrics Summary:\n",
      "                   Precision                     Recall                     \\\n",
      "Set                     Test   Train Validation    Test   Train Validation   \n",
      "Model                                                                        \n",
      "Balanced_Bagging      1.0000  0.9464     0.9375  0.6667  1.0000     0.8333   \n",
      "CatBoost              1.0000  0.8983     0.8421  0.7778  1.0000     0.8889   \n",
      "LightGBM              0.0039  0.0037     0.0035  1.0000  0.9811     0.8889   \n",
      "LogisticRegression    0.7778  0.7681     0.8000  0.7778  1.0000     0.8889   \n",
      "Random Forest         1.0000  0.8413     0.8824  0.7778  1.0000     0.8333   \n",
      "Stacking              0.9333  0.9138     0.8421  0.7778  1.0000     0.8889   \n",
      "XGBoost               1.0000  0.9464     0.8889  0.7778  1.0000     0.8889   \n",
      "\n",
      "                   Specificity                     \n",
      "Set                       Test   Train Validation  \n",
      "Model                                              \n",
      "Balanced_Bagging        1.0000  0.9999     0.9999  \n",
      "CatBoost                1.0000  0.9998     0.9997  \n",
      "LightGBM                0.5942  0.5888     0.5968  \n",
      "LogisticRegression      0.9996  0.9995     0.9996  \n",
      "Random Forest           1.0000  0.9997     0.9998  \n",
      "Stacking                0.9999  0.9999     0.9997  \n",
      "XGBoost                 1.0000  0.9999     0.9998  \n",
      "\n",
      "AUC Scores Summary:\n",
      "                    AUC_PR                    AUC_ROC                   \n",
      "Set                   Test   Train Validation    Test   Train Validation\n",
      "Model                                                                   \n",
      "Balanced_Bagging    0.7787  1.0000     0.8280  0.9156  1.0000     0.9438\n",
      "CatBoost            0.8313  1.0000     0.8617  0.9953  1.0000     0.9786\n",
      "LightGBM            0.0039  0.0037     0.0033  0.7971  0.7850     0.7429\n",
      "LogisticRegression  0.7672  0.9153     0.7885  0.8823  0.9999     0.9622\n",
      "Random Forest       0.7958  0.9918     0.8527  0.9141  1.0000     0.9430\n",
      "Stacking            0.7932  1.0000     0.8795  0.9890  1.0000     0.9433\n",
      "XGBoost             0.8373  1.0000     0.8783  0.9952  1.0000     0.9856\n",
      "\n",
      "Balanced Accuracy Summary:\n",
      "Set                   Test   Train  Validation\n",
      "Model                                         \n",
      "Balanced_Bagging    0.8333  1.0000      0.9166\n",
      "CatBoost            0.8889  0.9999      0.9443\n",
      "LightGBM            0.7971  0.7850      0.7429\n",
      "LogisticRegression  0.8887  0.9998      0.9443\n",
      "Random Forest       0.8889  0.9999      0.9166\n",
      "Stacking            0.8888  0.9999      0.9443\n",
      "XGBoost             0.8889  1.0000      0.9444\n",
      "\n",
      "🏆 Model Selection:\n",
      "   Best Model (F1-Binary): XGBoost\n",
      "   Best Model (AUC-PR): Stacking\n",
      "\n",
      "📊 Class Distribution Analysis:\n",
      "Training set class distribution:\n",
      "   Negative (0): 34,085 (99.84%)\n",
      "   Positive (1): 53 (0.16%)\n",
      "   Imbalance Ratio: 643.1:1\n",
      "\n",
      "⚖️ Computed Class Weights:\n",
      "   Negative (0): 0.501\n",
      "   Positive (1): 322.057\n",
      "\n",
      "💡 Key Insights:\n",
      "   • F1-Binary focuses on positive class performance\n",
      "   • AUC-PR is more informative than AUC-ROC for imbalanced data\n",
      "   • High specificity shows good negative class recognition\n",
      "   • Balance precision vs recall based on business needs\n",
      "   • Ensemble methods often provide more robust predictions\n"
     ]
    }
   ],
   "source": [
    "# Main training and evaluation code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "best_models = {}\n",
    "results = []\n",
    "detailed_results = []\n",
    "\n",
    "# Use stratified CV for better handling of imbalanced data\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, (model, params, scale) in models.items():\n",
    "    print(f\"\\nTraining and tuning {name}...\")\n",
    "    \n",
    "    try:\n",
    "        if scale:\n",
    "            # Apply scaling\n",
    "            scaler = RobustScaler()\n",
    "            X_train_used = scaler.fit_transform(clip_outliers(X_train))\n",
    "            X_val_used = scaler.transform(clip_outliers(X_val))\n",
    "            X_test_used = scaler.transform(clip_outliers(X_test))\n",
    "        else:\n",
    "            X_train_used, X_val_used, X_test_used = X_train, X_val, X_test\n",
    "        \n",
    "        # Handle models with and without hyperparameter tuning\n",
    "        if params:  # If there are parameters to tune\n",
    "            # Use stratified cross-validation and appropriate scoring for imbalanced data\n",
    "            grid = GridSearchCV(\n",
    "                model, \n",
    "                params, \n",
    "                cv=stratified_cv, \n",
    "                n_jobs=-1, \n",
    "                scoring='f1',  # F1 for binary classification\n",
    "                verbose=0,  # Reduced verbosity\n",
    "                error_score='raise'\n",
    "            )\n",
    "            \n",
    "            grid.fit(X_train_used, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            \n",
    "            print(f\"Best params: {grid.best_params_}\")\n",
    "            print(f\"Best CV score: {grid.best_score_:.4f}\")\n",
    "            \n",
    "        else:  # No hyperparameter tuning (e.g., for voting classifiers)\n",
    "            best_model = model\n",
    "            best_model.fit(X_train_used, y_train)\n",
    "            \n",
    "            # Calculate CV score manually for comparison\n",
    "            cv_scores = cross_val_score(best_model, X_train_used, y_train, \n",
    "                                       cv=stratified_cv, scoring='f1', n_jobs=-1)\n",
    "            print(f\"CV F1 score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        best_models[name] = best_model\n",
    "        \n",
    "        # Find optimal threshold using validation set for binary classification\n",
    "        try:\n",
    "            optimal_threshold = find_optimal_threshold(best_model, X_val_used, y_val, metric='f1')\n",
    "            print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not find optimal threshold for {name}: {e}\")\n",
    "            optimal_threshold = 0.5  # Default threshold\n",
    "        \n",
    "        # Evaluate on all sets\n",
    "        sets = {\n",
    "            \"Train\": (X_train_used, y_train),\n",
    "            \"Validation\": (X_val_used, y_val),\n",
    "            \"Test\": (X_test_used, y_test)\n",
    "        }\n",
    "        \n",
    "        for set_name, (X_set, y_set) in sets.items():\n",
    "            try:\n",
    "                eval_results = comprehensive_evaluation(\n",
    "                    best_model, X_set, y_set, optimal_threshold, set_name, name\n",
    "                )\n",
    "                \n",
    "                # Store basic results with binary metrics\n",
    "                results.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Set\": set_name,\n",
    "                    \"F1_Macro\": round(eval_results.get('f1_macro', 0), 4),\n",
    "                    \"F1_Binary\": round(eval_results.get('f1_binary', 0), 4),\n",
    "                    \"F1_Weighted\": round(eval_results.get('f1_weighted', 0), 4),\n",
    "                    \"Balanced_Accuracy\": round(eval_results.get('balanced_accuracy', 0), 4),\n",
    "                    \"AUC_ROC\": round(eval_results.get('auc_roc', 0), 4),\n",
    "                    \"AUC_PR\": round(eval_results.get('auc_pr', 0), 4),\n",
    "                    \"Precision\": round(eval_results.get('precision_binary', 0), 4),\n",
    "                    \"Recall\": round(eval_results.get('recall_binary', 0), 4),\n",
    "                    \"Specificity\": round(eval_results.get('specificity', 0), 4)\n",
    "                })\n",
    "                \n",
    "                # Store detailed results\n",
    "                detailed_results.append({\n",
    "                    'model': name,\n",
    "                    'set': set_name,\n",
    "                    'threshold': optimal_threshold,\n",
    "                    **eval_results\n",
    "                })\n",
    "                \n",
    "                # Print detailed report for validation set only\n",
    "                if set_name == \"Validation\":\n",
    "                    y_pred = eval_results['predictions']\n",
    "                    \n",
    "                    print(f\"\\n=== {name} - {set_name} Set Results ===\")\n",
    "                    print(f\"🎯 Threshold: {optimal_threshold:.4f}\")\n",
    "                    \n",
    "                    # Key binary metrics\n",
    "                    print(f\"\\n📈 Key Metrics:\")\n",
    "                    print(f\"   F1-Binary (Positive Class): {eval_results.get('f1_binary', 0):.4f}\")\n",
    "                    print(f\"   F1-Macro: {eval_results.get('f1_macro', 0):.4f}\")\n",
    "                    print(f\"   Balanced Accuracy: {eval_results.get('balanced_accuracy', 0):.4f}\")\n",
    "                    \n",
    "                    # Precision/Recall for positive class\n",
    "                    print(f\"\\n🔍 Positive Class Performance:\")\n",
    "                    print(f\"   Precision: {eval_results.get('precision_binary', 0):.4f}\")\n",
    "                    print(f\"   Recall (Sensitivity): {eval_results.get('recall_binary', 0):.4f}\")\n",
    "                    print(f\"   Specificity: {eval_results.get('specificity', 0):.4f}\")\n",
    "                    \n",
    "                    # AUC scores\n",
    "                    print(f\"\\n📊 AUC Scores:\")\n",
    "                    print(f\"   AUC-ROC: {eval_results.get('auc_roc', 0):.4f}\")\n",
    "                    print(f\"   AUC-PR: {eval_results.get('auc_pr', 0):.4f}\")\n",
    "                    \n",
    "                    # Confusion Matrix with labels\n",
    "                    print(f\"\\n🔢 Confusion Matrix:\")\n",
    "                    print(f\"   True Negatives:  {eval_results.get('true_negatives', 0):,}\")\n",
    "                    print(f\"   False Positives: {eval_results.get('false_positives', 0):,}\")\n",
    "                    print(f\"   False Negatives: {eval_results.get('false_negatives', 0):,}\")\n",
    "                    print(f\"   True Positives:  {eval_results.get('true_positives', 0):,}\")\n",
    "                    \n",
    "                    # Classification report\n",
    "                    try:\n",
    "                        present_classes = np.unique(np.concatenate([y_set, y_pred]))\n",
    "                        present_class_names = ['Negative', 'Positive']\n",
    "                        print(f\"\\nClassification Report:\")\n",
    "                        print(classification_report(\n",
    "                            y_set, y_pred,\n",
    "                            labels=present_classes,\n",
    "                            target_names=present_class_names,\n",
    "                            zero_division=0\n",
    "                        ))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not generate classification report: {e}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {name} on {set_name} set: {e}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create summary DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY OF BINARY CLASSIFICATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # F1 scores summary\n",
    "    print(\"\\nF1 Scores Summary:\")\n",
    "    try:\n",
    "        f1_summary = results_df.pivot_table(\n",
    "            index='Model', \n",
    "            columns='Set', \n",
    "            values=['F1_Binary', 'F1_Macro'], \n",
    "            aggfunc='first'\n",
    "        )\n",
    "        print(f1_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create F1 summary: {e}\")\n",
    "\n",
    "    # Key binary metrics summary\n",
    "    print(\"\\nKey Binary Metrics Summary:\")\n",
    "    try:\n",
    "        binary_metrics = results_df.pivot_table(\n",
    "            index='Model',\n",
    "            columns='Set', \n",
    "            values=['Precision', 'Recall', 'Specificity'],\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        print(binary_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create binary metrics summary: {e}\")\n",
    "\n",
    "    # AUC scores summary\n",
    "    print(\"\\nAUC Scores Summary:\")\n",
    "    try:\n",
    "        auc_summary = results_df.pivot_table(\n",
    "            index='Model',\n",
    "            columns='Set',\n",
    "            values=['AUC_ROC', 'AUC_PR'],\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        print(auc_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create AUC summary: {e}\")\n",
    "\n",
    "    # Balanced Accuracy summary  \n",
    "    print(\"\\nBalanced Accuracy Summary:\")\n",
    "    try:\n",
    "        balanced_acc_pivot = results_df.pivot(index='Model', columns='Set', values='Balanced_Accuracy')\n",
    "        print(balanced_acc_pivot)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create balanced accuracy summary: {e}\")\n",
    "\n",
    "    # Find best model based on validation metrics\n",
    "    try:\n",
    "        val_results = results_df[results_df['Set'] == 'Validation'].copy()\n",
    "        if not val_results.empty:\n",
    "            best_model_f1_binary = val_results.loc[val_results['F1_Binary'].idxmax(), 'Model']\n",
    "            best_model_auc_pr = val_results.loc[val_results['AUC_PR'].idxmax(), 'Model']\n",
    "\n",
    "            print(f\"\\n🏆 Model Selection:\")\n",
    "            print(f\"   Best Model (F1-Binary): {best_model_f1_binary}\")\n",
    "            print(f\"   Best Model (AUC-PR): {best_model_auc_pr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine best models: {e}\")\n",
    "\n",
    "    # Class imbalance analysis\n",
    "    print(f\"\\n📊 Class Distribution Analysis:\")\n",
    "    try:\n",
    "        print(f\"Training set class distribution:\")\n",
    "        train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "        print(f\"   Negative (0): {train_dist[0]:,} ({train_dist[0]/len(y_train)*100:.2f}%)\")\n",
    "        print(f\"   Positive (1): {train_dist[1]:,} ({train_dist[1]/len(y_train)*100:.2f}%)\")\n",
    "        print(f\"   Imbalance Ratio: {train_dist[0]/train_dist[1]:.1f}:1\")\n",
    "\n",
    "        # Calculate class weights for reference\n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "            print(f\"\\n⚖️ Computed Class Weights:\")\n",
    "            print(f\"   Negative (0): {class_weights[0]:.3f}\")\n",
    "            print(f\"   Positive (1): {class_weights[1]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform class distribution analysis: {e}\")\n",
    "\n",
    "    # Performance insights for imbalanced data\n",
    "    print(f\"\\n💡 Key Insights:\")\n",
    "    print(f\"   • F1-Binary focuses on positive class performance\")\n",
    "    print(f\"   • AUC-PR is more informative than AUC-ROC for imbalanced data\")\n",
    "    print(f\"   • High specificity shows good negative class recognition\")\n",
    "    print(f\"   • Balance precision vs recall based on business needs\")\n",
    "    print(f\"   • Ensemble methods often provide more robust predictions\")\n",
    "\n",
    "else:\n",
    "    print(\"No results were generated. Please check for errors above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
